{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GedsgoyBHjV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import os\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Veri artırma için ImageDataGenerator**"
      ],
      "metadata": {
        "id": "4t6urb60BL1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    rescale=1./255  # Normalizasyonu burada yapabiliriz\n",
        ")"
      ],
      "metadata": {
        "id": "AbcvOJnWBPqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **VggNet mimarisi**"
      ],
      "metadata": {
        "id": "Z5TBLGYRBSMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vggnet_model(input_shape, num_classes):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Input(shape=input_shape))\n",
        "\n",
        "\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(4096, activation='relu'))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(4096, activation='relu'))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "Hb1G9j7-BTK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Veri işlemesi ve train-test ayrımı**"
      ],
      "metadata": {
        "id": "a77mxXimBWCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/kaggle/input/crop-diseases-classification/Data/train_images'\n",
        "labels_csv_path = '/kaggle/input/crop-diseases-classification/Data/train.csv'\n",
        "\n",
        "labels_df = pd.read_csv(labels_csv_path)\n",
        "\n",
        "image_filenames = labels_df['image_id'].tolist()\n",
        "labels = labels_df['label'].tolist()\n",
        "\n",
        "image_paths = [os.path.join(data_dir, filename) for filename in image_filenames]\n",
        "\n",
        "valid_image_paths = []\n",
        "valid_labels = []\n",
        "\n",
        "for index, row in labels_df.iterrows():\n",
        "    image_path = os.path.join(data_dir, row['image_id'])\n",
        "    if os.path.exists(image_path):\n",
        "        valid_image_paths.append(image_path)\n",
        "        valid_labels.append(row['label'])\n",
        "\n",
        "if len(valid_image_paths) == 0:\n",
        "    raise ValueError(\"No valid image paths found. Please check your file paths and ensure the images exist.\")\n",
        "\n",
        "train_paths, test_paths, train_labels, test_labels = train_test_split(valid_image_paths, valid_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "def preprocess_image(image_path, label, img_size=(128, 128)):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, img_size)\n",
        "    image = image / 255.0  # Normalizasyon\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "hmD001ctBZmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Görüntü verilerini artırmak**"
      ],
      "metadata": {
        "id": "hgE62b-IBdyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augment(image, label):\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
        "    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "xn2FKg7BBhii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Veri seti oluşturma ve ön işleme**"
      ],
      "metadata": {
        "id": "7I3mt3-IBj_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_paths, test_labels))\n",
        "\n",
        "train_dataset = train_dataset.map(lambda x, y: preprocess_image(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_dataset = train_dataset.map(lambda x, y: augment(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.map(lambda x, y: preprocess_image(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "SHUFFLE_BUFFER_SIZE = 500\n",
        "\n",
        "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "ZdP9Cg30BmP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Oluşturma ve Derleme**"
      ],
      "metadata": {
        "id": "RGJn23NqBsIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (128, 128, 3)\n",
        "num_classes = len(set(valid_labels))\n",
        "\n",
        "model = vggnet_model(input_shape, num_classes)\n",
        "initial_learning_rate = 0.00001\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "T8tdZmEqBu4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Geri çağırma (callback)**"
      ],
      "metadata": {
        "id": "xhSu_r6SByDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)"
      ],
      "metadata": {
        "id": "2e7ZwpMNBvCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modeli eğitme**"
      ],
      "metadata": {
        "id": "rWyj1dKaB29x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_dataset, epochs=30, validation_data=test_dataset, callbacks=[early_stopping, reduce_lr])"
      ],
      "metadata": {
        "id": "79hVVGPzB5Ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Loss grafiği\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Accuracy grafiği\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "WS-t7bKPCG2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "y_pred = model.predict(test_dataset)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Gerçek etiketleri al\n",
        "y_true = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "\n",
        "# Doğruluk (Accuracy)\n",
        "accuracy = accuracy_score(y_true, y_pred_classes)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Kesinlik (Precision)\n",
        "precision = precision_score(y_true, y_pred_classes, average='weighted')\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "\n",
        "# Geri Çağırma (Recall)\n",
        "recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "# F-ölçüsü (F1-Score)\n",
        "f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
        "print(f\"F1-Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "QEWukItGCIj2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}