{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcBe5gcS4R_z"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import os\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Veri artırma için ImageDataGenerator**"
      ],
      "metadata": {
        "id": "FcBA-xcU4Umh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")"
      ],
      "metadata": {
        "id": "UdKT0CXZ5sNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GoogleNet Mimarisi**"
      ],
      "metadata": {
        "id": "9uvje9am5vmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inception_module(x, filters):\n",
        "    f1, f2_in, f2_out, f3_in, f3_out, f4_out = filters\n",
        "\n",
        "    conv1 = layers.Conv2D(f1, (1, 1), padding='same', activation='relu')(x)\n",
        "\n",
        "    conv3 = layers.Conv2D(f2_in, (1, 1), padding='same', activation='relu')(x)\n",
        "    conv3 = layers.Conv2D(f2_out, (3, 3), padding='same', activation='relu')(conv3)\n",
        "\n",
        "    conv5 = layers.Conv2D(f3_in, (1, 1), padding='same', activation='relu')(x)\n",
        "    conv5 = layers.Conv2D(f3_out, (5, 5), padding='same', activation='relu')(conv5)\n",
        "\n",
        "    pool = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
        "    pool = layers.Conv2D(f4_out, (1, 1), padding='same', activation='relu')(pool)\n",
        "\n",
        "    return layers.concatenate([conv1, conv3, conv5, pool], axis=-1)\n",
        "\n",
        "def googlenet_model(input_shape, num_classes):\n",
        "    input_layer = layers.Input(shape=input_shape)\n",
        "\n",
        "    x = layers.Conv2D(64, (7, 7), strides=(2, 2), padding='same', activation='relu')(input_layer)\n",
        "    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "    x = layers.Conv2D(64, (1, 1), padding='same', activation='relu')(x)\n",
        "    x = layers.Conv2D(192, (3, 3), padding='same', activation='relu')(x)\n",
        "    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "\n",
        "    x = inception_module(x, [64, 96, 128, 16, 32, 32])\n",
        "    x = inception_module(x, [128, 128, 192, 32, 96, 64])\n",
        "    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "\n",
        "    x = inception_module(x, [192, 96, 208, 16, 48, 64])\n",
        "    x = inception_module(x, [160, 112, 224, 24, 64, 64])\n",
        "    x = inception_module(x, [128, 128, 256, 24, 64, 64])\n",
        "    x = inception_module(x, [112, 144, 288, 32, 64, 64])\n",
        "    x = inception_module(x, [256, 160, 320, 32, 128, 128])\n",
        "    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "\n",
        "    x = inception_module(x, [256, 160, 320, 32, 128, 128])\n",
        "    x = inception_module(x, [384, 192, 384, 48, 128, 128])\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "    output_layer = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(input_layer, output_layer)\n",
        "    return model"
      ],
      "metadata": {
        "id": "zqLO0qnh5yS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Veri işlemesi ve train-test ayrımı**"
      ],
      "metadata": {
        "id": "c5V8iY4D50QG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/kaggle/input/crop-diseases-classification/Data/train_images'\n",
        "labels_csv_path = '/kaggle/input/crop-diseases-classification/Data/train.csv'\n",
        "\n",
        "labels_df = pd.read_csv(labels_csv_path)\n",
        "\n",
        "image_filenames = labels_df['image_id'].tolist()\n",
        "labels = labels_df['label'].tolist()\n",
        "\n",
        "image_paths = [os.path.join(data_dir, filename) for filename in image_filenames]\n",
        "\n",
        "valid_image_paths = []\n",
        "valid_labels = []\n",
        "\n",
        "for index, row in labels_df.iterrows():\n",
        "    image_path = os.path.join(data_dir, row['image_id'])\n",
        "    if os.path.exists(image_path):\n",
        "        valid_image_paths.append(image_path)\n",
        "        valid_labels.append(row['label'])\n",
        "\n",
        "if len(valid_image_paths) == 0:\n",
        "    raise ValueError(\"No valid image paths found. Please check your file paths and ensure the images exist.\")\n",
        "\n",
        "train_paths, test_paths, train_labels, test_labels = train_test_split(valid_image_paths, valid_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "def preprocess_image(image_path, label, img_size=(224, 224)):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, img_size)\n",
        "    image = image / 255.0\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "rGTx-5zM52pP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Görüntü verilerini artırmak**"
      ],
      "metadata": {
        "id": "PoDBXvss54Ym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augment(image, label):\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
        "    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "yMPQOjTj56iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Veri seti oluşturma ve ön işleme**"
      ],
      "metadata": {
        "id": "zYGog57258XG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_paths, test_labels))\n",
        "\n",
        "train_dataset = train_dataset.map(lambda x, y: preprocess_image(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_dataset = train_dataset.map(lambda x, y: augment(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.map(lambda x, y: preprocess_image(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "SHUFFLE_BUFFER_SIZE = 1000\n",
        "\n",
        "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "JI3bD18n5-hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Oluşturma ve Derleme**"
      ],
      "metadata": {
        "id": "A07WjS1Y6KL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (224, 224, 3)\n",
        "num_classes = len(set(valid_labels))\n",
        "\n",
        "model = googlenet_model(input_shape, num_classes)\n",
        "initial_learning_rate = 0.0001\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "for image_batch, label_batch in train_dataset.take(1):\n",
        "    print(f\"Image batch shape: {image_batch.shape}\")\n",
        "    print(f\"Label batch shape: {label_batch.shape}\")\n",
        "\n",
        "for image_batch, label_batch in test_dataset.take(1):\n",
        "    print(f\"Image batch shape: {image_batch.shape}\")\n",
        "    print(f\"Label batch shape: {label_batch.shape}\")\n"
      ],
      "metadata": {
        "id": "PDTv7_NK6MME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Geri çağırma (callback)**"
      ],
      "metadata": {
        "id": "2QxnryIl6N7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)"
      ],
      "metadata": {
        "id": "90aYW0BU6P5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modeli eğitme**"
      ],
      "metadata": {
        "id": "IME89S9N6RhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_dataset, epochs=30, validation_data=test_dataset, callbacks=[early_stopping, reduce_lr])"
      ],
      "metadata": {
        "id": "XiQ0KDOP6TZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modeli test veri kümesi üzerinde değerlendirme**"
      ],
      "metadata": {
        "id": "ZyIBeaWK6VRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "def plot_history(history):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Loss grafiği\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Accuracy grafiği\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Metrikleri çiz\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "D7npe6E66XLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "y_pred = model.predict(test_dataset)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Gerçek etiketleri al\n",
        "y_true = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "\n",
        "# Doğruluk (Accuracy)\n",
        "accuracy = accuracy_score(y_true, y_pred_classes)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Kesinlik (Precision)\n",
        "precision = precision_score(y_true, y_pred_classes, average='weighted')\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "\n",
        "# Geri Çağırma (Recall)\n",
        "recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "# F-ölçüsü (F1-Score)\n",
        "f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
        "print(f\"F1-Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "awxzQSYU6YvD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}